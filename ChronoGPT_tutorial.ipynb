{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/LinyingLyu/ChronoGPT.git\n",
        "%cd ChronoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEdSLfsukXB8"
      },
      "source": [
        "# Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQSKn4CflYh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from huggingface_hub import HfApi, login\n",
        "from ChronoGPT_inference import *\n",
        "import gc\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cache_dir = 'cache'  # Update this path as needed\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "max_length = 30\n",
        "num_return_sequences = 5\n",
        "seed = 11111\n",
        "\n",
        "# -------------------------- Load Model --------------------------\n",
        "repo_id = \"manelalab/chrono-gpt-v1-20241231\"\n",
        "config_path = hf_hub_download(repo_id=repo_id, filename=\"config.pt\", cache_dir=cache_dir)\n",
        "bin_path = hf_hub_download(repo_id=repo_id, filename=\"pytorch_model.bin\", cache_dir=cache_dir)\n",
        "config = torch.load(config_path, map_location='cpu')\n",
        "print(f\"Model config: {config}\")\n",
        "model = ChronoGPT(**config)\n",
        "model = model.to(device)\n",
        "model = model.half()\n",
        "\n",
        "state_dict = torch.load(bin_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "del state_dict\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ------------------------ Prepare Input -------------------------\n",
        "prompt = \"Hello, I am a language model,\"\n",
        "tokens = tokenizer.encode(prompt)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
        "tokens = tokens.repeat(num_return_sequences, 1).to(device)\n",
        "\n",
        "# -------------------- Sampling Initialization -------------------\n",
        "xgen = tokens.clone()\n",
        "sample_rng = torch.Generator(device=device)\n",
        "sample_rng.manual_seed(seed)\n",
        "\n",
        "# ------------------------- Text Generation -----------------------\n",
        "while xgen.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(xgen)\n",
        "\n",
        "        logits = logits[:, -1, :]  # Last token logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 30, dim=-1)\n",
        "\n",
        "        sampled_idx = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
        "        next_token = torch.gather(topk_indices, -1, sampled_idx)\n",
        "\n",
        "        xgen = torch.cat([xgen, next_token], dim=1)\n",
        "\n",
        "\n",
        "# ------------------------- Decode Output -------------------------\n",
        "for i in range(num_return_sequences):\n",
        "    decoded_tokens = xgen[i, :max_length].tolist()\n",
        "    decoded_text = tokenizer.decode(decoded_tokens)\n",
        "    print(f\"Rank sample {i}:\\n{decoded_text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsqTT5oTksLV"
      },
      "source": [
        "# Embeddings extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu2dYzx-gYQB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from huggingface_hub import HfApi, login\n",
        "from ChronoGPT_inference import *\n",
        "\n",
        "# ----------------------------- Setup -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cache_dir = 'cache'  # Update this path as needed\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# -------------------------- Load Model --------------------------\n",
        "repo_id = \"manelalab/chrono-gpt-v1-20241231\"\n",
        "config_path = hf_hub_download(repo_id=repo_id, filename=\"config.pt\", cache_dir=cache_dir)\n",
        "bin_path = hf_hub_download(repo_id=repo_id, filename=\"pytorch_model.bin\", cache_dir=cache_dir)\n",
        "config = torch.load(config_path, map_location='cpu')\n",
        "print(f\"Model config: {config}\")\n",
        "model = ChronoGPT(**config)\n",
        "model = model.to(device)\n",
        "model = model.half()\n",
        "\n",
        "state_dict = torch.load(bin_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "del state_dict\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------- Embedding Generation ---------------------\n",
        "text = \"Obviously, the time continuum has been disrupted, creating a new temporal event sequence resulting in this alternate reality.\"\n",
        "\n",
        "inputs = torch.tensor(tokenizer.encode(text))[:max_length].reshape(1,-1).to(device)\n",
        "logits, emb = model(inputs)\n",
        "print('Dimension of embeddings:', emb[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq-GTxA0i4K2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
