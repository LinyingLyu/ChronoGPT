{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/LinyingLyu/ChronoGPT.git\n",
        "%cd ChronoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEdSLfsukXB8"
      },
      "source": [
        "# Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQSKn4CflYh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from huggingface_hub import HfApi, login\n",
        "from ChronoGPT_inference import *\n",
        "\n",
        "# ----------------------------- Setup -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cache_dir = 'cache'  # Update this path as needed\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "max_length = 50\n",
        "num_return_sequences = 5\n",
        "seed = 666\n",
        "\n",
        "# -------------------------- Load Model --------------------------\n",
        "model = ChronoGPT_xl.from_pretrained(\n",
        "    \"manelalab/chrono-gpt-v1-20241231\",\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_dir\n",
        ").to(device) # Replace ChronoGPT with ChronoGPT_xl when loading 1.5B models\n",
        "\n",
        "# ------------------------ Prepare Input -------------------------\n",
        "prompt = \"Hello, I am a language model,\"\n",
        "tokens = tokenizer.encode(prompt)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0)\n",
        "tokens = tokens.repeat(num_return_sequences, 1).to(device)\n",
        "\n",
        "# -------------------- Sampling Initialization -------------------\n",
        "xgen = tokens.clone()\n",
        "sample_rng = torch.Generator(device=device)\n",
        "sample_rng.manual_seed(seed)\n",
        "\n",
        "# ------------------------- Text Generation -----------------------\n",
        "while xgen.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "            logits, _ = model(xgen)\n",
        "\n",
        "        logits = logits[:, -1, :]  # Last token logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "\n",
        "        sampled_idx = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
        "        next_token = torch.gather(topk_indices, -1, sampled_idx)\n",
        "\n",
        "        xgen = torch.cat([xgen, next_token], dim=1)\n",
        "\n",
        "# ------------------------- Decode Output -------------------------\n",
        "for i in range(num_return_sequences):\n",
        "    decoded_tokens = xgen[i, :max_length].tolist()\n",
        "    decoded_text = tokenizer.decode(decoded_tokens)\n",
        "    print(f\"Rank sample {i}:\\n{decoded_text}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsqTT5oTksLV"
      },
      "source": [
        "# Embeddings extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu2dYzx-gYQB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from huggingface_hub import HfApi, login\n",
        "from ChronoGPT_inference import *\n",
        "\n",
        "# ----------------------------- Setup -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cache_dir = 'cache'  # Update this path as needed\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# -------------------------- Load Model --------------------------\n",
        "model = ChronoGPT_xl.from_pretrained(\n",
        "    \"manelalab/chrono-gpt-v1-20241231\",\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_dir\n",
        ").to(device) # Replace ChronoGPT with ChronoGPT_xl when loading 1.5B models\n",
        "\n",
        "# ----------------------- Embedding Generation ---------------------\n",
        "text = \"Obviously, the time continuum has been disrupted, creating a new temporal event sequence resulting in this alternate reality.\"\n",
        "\n",
        "inputs = torch.tensor(tokenizer.encode(text))[:max_length].reshape(1,-1).to(device)\n",
        "logits, emb = model(inputs)\n",
        "print('Dimension of embeddings:', emb[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq-GTxA0i4K2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
