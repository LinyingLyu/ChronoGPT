{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awG0MvLI0MME"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/LinyingLyu/ChronoGPT.git\n",
        "%cd ChronoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEdSLfsukXB8"
      },
      "source": [
        "# Load ChronoGPT-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTQSKn4CflYh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "from huggingface_hub import hf_hub_download\n",
        "from ChronoGPT_instruct import *\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Load model\n",
        "repo_id = \"manelalab/chrono-gpt-instruct-v1-20241231\"\n",
        "config_path = hf_hub_download(repo_id=repo_id, filename=\"config.pt\")\n",
        "bin_path = hf_hub_download(repo_id=repo_id, filename=\"pytorch_model.bin\")\n",
        "\n",
        "config = torch.load(config_path, map_location='cpu')\n",
        "model = ChronoGPT(**config).to(device).half()\n",
        "model.load_state_dict(torch.load(bin_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"ChronoGPT-instruct successfully loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsqTT5oTksLV"
      },
      "source": [
        "# Instruction Following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu2dYzx-gYQB"
      },
      "outputs": [],
      "source": [
        "user_input = \"Who is the first man landing on the moon?\"\n",
        "response = extract_response(user_input, model=model, tokenizer=tokenizer, device=device, max_tokens=128, temperature=0.0)\n",
        "\n",
        "print(f\"You: {user_input}\")\n",
        "print(f\"\\nModel: {response}\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
